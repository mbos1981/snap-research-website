<div class="container content">
  <p>
    The 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2023) is
    right around the corner, bringing in experts in data science and machine learning to share their
    latest research. At KDD 2023, a premier data mining and machine learning conference, Snap
    Research is contributing across various workshops, tutorials, accepted papers and invited talks,
    furthering knowledge and advancements in graph mining, causal inference, and related fields.
    Moreover, this year, Snap Research supported KDD through sponsorship and Organizing
    Committee service. See below for the highlights.
  </p>
  <br/>
  
  <p class="sub-section">
    <b>Workshops</b>
  </p>

  <p>
    <a href="http://www.mlgworkshop.org/2023/"><strong>Mining and Learning with Graphs Workshop</strong></a>
    <br/>
    <em><strong>Neil Shah</strong>, Shobeir Fakhraei, Da Zheng, Bahare Fatemi, Leman Akoglu</em><br/>
    This workshop provides a platform for researchers and practitioners to discuss the latest
    developments in graph-based machine learning via keynotes, invited talks and poster
    presentations.
  </p>

  <p>
    <a href="https://causal-machine-learning.github.io/kdd2023-workshop/"><strong>Causal Inference and Machine Learning in Practice</strong></a><br/>
    <em>Chu Wang, Yingfei Wang, Xinwei Ma, Zeyu Zheng, <strong>Jing Pan</strong>, Yifeng Wu, Huigang Chen, Totte</em><br/>
    Harinen, Paul Lo, Jeong-Yoon Lee, Zhenyu Zhao, Fabio Vera, Eleanor Dillon, Keith Battocchi
    This workshop brings together researchers and practitioners to share experiences and insights
    from applying causal inference and machine learning techniques to real-world problems in areas
    of product, brand, policy and beyond.
  </p>
  <br/>
  
  <p class="sub-section">
    <b>Tutorials</b>
  </p>

  <p>
    <a href="https://sites.google.com/ncsu.edu/gnnkdd2023tutorial/home?pli=1"><strong>Large-Scale Graph Neural Networks: The Past and New Frontiers</strong></a><br/>
    <em>Rui Xue, Haoyu Han, <strong>Tong Zhao, Neil Shah</strong>, Jiliang Tang, Xiaorui Liu</em><br/>
    This tutorial overviews a body of work in training and inference of graph neural networks at scale,
    including lazy propagation, piecewise training, condensation, distillation, pre-training and model
    pruning.
  </p>
  <br/>
  
  <p class="sub-section">
    <b>Accepted Papers</b>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2306.06936.pdf"><strong>CARL-G: Clustering-Accelerated Representation Learning on Graphs</strong></a><br/>
    <em>William Shiao, Uday Saini, <strong>Yozen Liu, Tong Zhao, Neil Shah</strong>, Evangelos Papalexakis</em><br/>
    We propose a new framework for graph self-supervised learning by adapting clustering validation
    indices as loss functions, with over 79x training speedup and no performance degradation. [<a href="https://research.snap.com/news/news-one.html#accelerating-graph-2023">blog</a>]
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2305.12087.pdf"><strong>Semi-supervised Graph Imbalanced Regression</strong></a><br/>
    <em>Gang Liu, <strong>Tong Zhao</strong>, Eric Inae, Tengfei Luo, Meng Jiang</em><br/>
    We propose a semi-supervised framework for graph regression tasks, which uses pseudo-
    labeling and latent space augmentation to achieve better data balance and reduce model bias,
    with promising results in 7 benchmarks.
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2106.04486.pdf"><strong>Sketch-based Anomaly Detection in Streaming Graphs</strong></a><br/>
    <em>Siddharth Bhatia, Mohit Wadhwa, Kenji Kawaguchi, <strong>Neil Shah</strong>, Philip Yu, Bryan Hooi</em><br/>
    We propose a first-of-its-kind constant-time and constant-space approach for detecting graph
    anomalies in the streaming setting using higher-order sketching.
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2302.05549.pdf"><strong>Balancing Approach for Causal Inference at Scale</strong></a><br/>
    <em><strong>Sicheng Lin</strong>, <strong>Meng Xu</strong>, <strong>Xi Zhang</strong>, Shih-Kang Chao, Ying-Kai Huang, <strong>Xiaolin Shi</strong></em><br/>
    We present two scalable algorithms for balancing approaches to solve causal inference problems
    at scale of 10 million units, which are deployed in an end-to-end system at Snap and significantly
    reduce both bias and variance in causal effect estimation.
  </p>
  <br/>
  
  <p class="sub-section">
    <b>Organization</b>
  </p>
  <p>
    <a href="https://kdd.org/kdd2023/organizers/"><strong>Hands-On Tutorials</strong></a><br/>
    <em><strong>Neil Shah</strong>, Lei Li, Huan Sun</em><br/>
    Hands-on Tutorials feature in-depth use of cutting-edge systems and relevant tools to the data
    mining and machine learning community.
  </p>
  <br/>
  
  <p class="sub-section">
    <b>Invited Talks</b>
  </p>
  <p>
    <a href="https://kdd.org/kdd2023/ads-invited-speakers/"><strong>Challenges of Online Measurement for Mobile Apps: A Causal Inference Perspective</strong></a><br/>
    <em><strong>Xiaolin Shi</strong></em><br/>
    We present an overview of causal inference as it applies to measurements in a mobile app
    setting, and discuss how we can handle several challenges of going beyond randomization when
    A/B tests are not feasible, observing heterogeneous treatment effects when averages are
    insufficient, and treating app performance metrics as a focal point instead of only a guardrail. This
    is a KDD Applied Data Science Invited Talk.
  </p>

  <p>
    <a href="https://graph-learning-benchmarks.github.io/glb2023"><strong>Graph Learning Benchmarks Panel</strong></a><br/>
    <em><strong>Neil Shah</strong></em><br/>
    We will discuss challenges and pain points in benchmarking algorithms and models in graph
    machine learning community, and touch on avenues to improve benchmarking going forward.
    This is a panel discussion at the KDD Workshop on Graph Learning Benchmarks.
  </p>

  <p>
    <a href="https://knowledge-nlp.github.io/kdd2023/keynote.html"><strong>The Importance of "In the Moment" Knowledge (KnowledgeNLP Workshop)</strong></a><br/>
    <em><strong>Francesco Barbieri</strong></em><br/>
    We will discuss why "in the moment" knowledge, like time, location, and weather data, is crucial 
    for improving NLP models. Given that posts on social platforms often contain short and 
    challenging-to-understand text, integrating contextual information helps us better interpret 
    these posts and represent the user's current state.
  </p>

  <br/>
  
  <p class="sub-section">
    <b>Sponsorship</b>
  </p>
  
  <p>
    <a href="https://kdd.org/kdd2023/sponsors/"><strong>KDD Undergraduate Consortium</strong></a><br/>
    Snap Research will sponsor the KDD Undergraduate Consortium, which supports the
    participation of undergraduate students, providing them with an opportunity to engage with
    experts, attend workshops, and expand their knowledge in the field of data science.
  </p>

  <p>
    Many folks from Snap will be at KDD this year -- please donâ€™t be shy and come check out our
work!
  </p>
</div>
</div>